{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, layer_dims):\n",
    "        print(\"NeuralNet class started\")\n",
    "        self.X_train_org, self.Y_train_org, self.X_test_org,self.Y_test_org, self.classes = self.dataset_load()\n",
    "        self.m_train = self.X_train_org.shape[0]\n",
    "        self.m_test = self.X_test_org.shape[0]\n",
    "        \n",
    "        print(\"size of the dataset:\",+self.m_train,\"test set:\",+self.m_test)\n",
    "        self.X_flatten_train = self.X_train_org.reshape(self.m_train, -1).T\n",
    "        print(\"sanity check after flatten\", str(self.X_flatten_train[0:5,0]))\n",
    "        self.X_train = self.X_flatten_train/255\n",
    "        self.X_flatten_test = (self.X_test_org.reshape(self.m_test, -1).T)\n",
    "        self.X_test = self.X_flatten_test/255\n",
    "        print(\"size of X_train after flatten and normalization\",self.X_train.shape,\"size of Y\",self.Y_train_org.shape)\n",
    "        self.layer_dims = layer_dims\n",
    "        self.layers = len(layer_dims)-1\n",
    "        print(\"Layers in the network: \", self.layers)\n",
    "        \n",
    "    def wt_init(self):\n",
    "        print(\"initializing wieghts\")\n",
    "        np.random.seed(3)\n",
    "        self.w = []\n",
    "        self.b = []\n",
    "        for i in range(1,self.layers+1):\n",
    "            print(\"i = \", i)\n",
    "            w = np.random.randn(self.layer_dims[i],self.layer_dims[i-1])*0.01\n",
    "            b = np.zeros((self.layer_dims[i],1))\n",
    "            self.w.append(w)\n",
    "            self.b.append(b)\n",
    "        \n",
    "        for i in range(0,self.layers):\n",
    "            print(i)\n",
    "            print(\"size of w:\",self.w[i].shape)\n",
    "            #print(self.w[i])\n",
    "             \n",
    "    def dataset_load(self):\n",
    "        print(\"loading the dataset\")\n",
    "        train_dataset = h5py.File('../../DL/Course_1_NeuralNetwork_DeepLearning/Week 2/Logistic Regression as a Neural Network/datasets/train_catvnoncat.h5', \"r\")\n",
    "        train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "        train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "        test_dataset = h5py.File('../../DL/Course_1_NeuralNetwork_DeepLearning/Week 2/Logistic Regression as a Neural Network/datasets/test_catvnoncat.h5', \"r\")\n",
    "        test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "        test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "        classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "\n",
    "        train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "        test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "        return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "        \n",
    "    def wt_train_deep(self, Type):\n",
    "        if Type == \"train\":\n",
    "            A = self.X_train\n",
    "        else:\n",
    "            A = self.X_test\n",
    "            \n",
    "        caches = []\n",
    "        for i in range(0,self.layers-1):\n",
    "            A_prev = A\n",
    "            A, activation_cache = self.wt_train(A_prev,self.w[i],self.b[i], activation = \"relu\")\n",
    "            caches.append(activation_cache)\n",
    "            \n",
    "        AL, activation_cache = self.wt_train(A,self.w[self.layers-1],self.b[self.layers-1], activation = \"sigmoid\")\n",
    "        caches.append(activation_cache)\n",
    "        \n",
    "        return AL, caches\n",
    "    \n",
    "    def wt_train(self,a_prev,w,b, activation):\n",
    "        #print(\"training the model\")\n",
    "        z = np.dot(w,a_prev)+ b\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            y_hat = relu(z)\n",
    "        else:\n",
    "            y_hat = sigmoid(z)\n",
    "            \n",
    "        cache = (a_prev,w,b,z)    \n",
    "            \n",
    "        return y_hat, cache\n",
    "    \n",
    "    def compute_cost(self, AL):\n",
    "        \n",
    "        #cost = (-1/self.m_train)*np.sum(np.dot(self.Y_train_org,np.log(AL.T))+np.dot((1-self.Y_train_org),np.log(1-AL.T)))\n",
    "        cost = (-1/self.m_train)*(np.dot(self.Y_train_org,np.log(AL.T))+np.dot((1-self.Y_train_org),np.log(1-AL.T)))\n",
    "        \n",
    "        cost = np.squeeze(cost)\n",
    "        return cost\n",
    "    \n",
    "    def backward_deep(self, AL, caches):\n",
    "        grades = {}\n",
    "        m = AL.shape[1]\n",
    "        #y = Y.reshape(AL.shape)\n",
    "        dAL = -(np.divide(self.Y_train_org,AL) - np.divide(1-self.Y_train_org,1-AL))\n",
    "        \n",
    "        current_caches = self.backward_activation(dAL, caches[self.layers-1], activation = \"sigmoid\")\n",
    "        grades[\"dA\" + str(self.layers-1)],grades[\"dW\" + str(self.layers)],grades[\"db\" + str(self.layers)] = current_caches[0],current_caches[1],current_caches[2]\n",
    "        \n",
    "        # loop from 2 to 0\n",
    "        for l in reversed(range(self.layers-1)):\n",
    "            current_caches = self.backward_activation(grades[\"dA\" + str(l+1)], caches[l], activation = \"relu\")\n",
    "            grades[\"dA\" + str(l)],grades[\"dW\" + str(l+1)],grades[\"db\" + str(l+1)] = current_caches[0],current_caches[1],current_caches[2]\n",
    "        \n",
    "        return grades\n",
    "    \n",
    "    def backward_activation(self, dA, caches, activation):\n",
    "        \n",
    "        if activation == \"relu\":\n",
    "            dZ = relu_backward(dA, caches)\n",
    "            dA_prev, dW, db = self.backward_linear(dZ, caches)\n",
    "        else:\n",
    "            dZ = sigmoid_backward(dA, caches)\n",
    "            dA_prev, dW, db = self.backward_linear(dZ, caches)\n",
    "        \n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    def backward_linear(self, dZ, caches):\n",
    "        A_prev, w, b, z = caches\n",
    "        m = A_prev.shape[1]\n",
    "        \n",
    "        dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "        db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "        dA_prev = np.dot(w.T,dZ)\n",
    "        \n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    def update_params(self, grades, learning_rate):\n",
    "        \n",
    "        for i in range(self.layers):\n",
    "            #print(\"updating param: i \", i)\n",
    "            self.w[i] = self.w[i] - learning_rate*grades[\"dW\" + str(i+1)]\n",
    "            self.b[i] = self.b[i] - learning_rate*grades[\"db\" + str(i+1)]\n",
    "            \n",
    "    def model(self,learning_rate, iteration):\n",
    "        self.wt_init()\n",
    "        costs = []\n",
    "        for i in range(iteration):\n",
    "            AL, caches = self.wt_train_deep(\"train\")\n",
    "            #print(AL)\n",
    "            cost = self.compute_cost(AL)\n",
    "            grades = self.backward_deep(AL,caches)\n",
    "            self.update_params(grades, learning_rate)\n",
    "            \n",
    "            if i%100 == 0:\n",
    "                print(\"cost = \", cost)\n",
    "                costs.append(cost)\n",
    "                \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()    \n",
    "        \n",
    "    def predict(self,Type):\n",
    "        AL, cachs = self.wt_train_deep(Type)\n",
    "        y_prediction = np.zeros((1, AL.shape[1]))\n",
    "        \n",
    "        for i in range(AL.shape[1]):\n",
    "            if AL[0,i] > 0.5:\n",
    "                y_prediction[0,i] = 1\n",
    "            else:\n",
    "                y_prediction[0,i] = 0\n",
    "            \n",
    "        return y_prediction    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    A = np.maximum(0,z)\n",
    "    assert(A.shape == z.shape)\n",
    "    return A\n",
    "\n",
    "def sigmoid(z):\n",
    "    A = 1/(1+np.exp(-z))\n",
    "    return A\n",
    "\n",
    "def relu_backward(dA, caches):\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    a_prev,w,b,z = caches    \n",
    "    #print(dA.shape, dZ.shape, z.shape, a_prev.shape, w.shape)\n",
    "    # When z <= 0, you should set dz to 0 as well.\n",
    "    dZ[z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, caches):\n",
    "    a_prev,w,b,z = caches\n",
    "    \n",
    "    s = sigmoid(z)\n",
    "    dZ = dA * s * (1-s)\n",
    "    #print(dA.shape, dZ.shape, z.shape, a_prev.shape, w.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet class started\n",
      "loading the dataset\n",
      "size of the dataset: 209 test set: 50\n",
      "sanity check after flatten [17 31 56 22 33]\n",
      "size of X_train after flatten and normalization (12288, 209) size of Y (1, 209)\n",
      "Layers in the network:  4\n",
      "initializing wieghts\n",
      "i =  1\n",
      "i =  2\n",
      "i =  3\n",
      "i =  4\n",
      "0\n",
      "size of w: (20, 12288)\n",
      "1\n",
      "size of w: (7, 20)\n",
      "2\n",
      "size of w: (5, 7)\n",
      "3\n",
      "size of w: (1, 5)\n",
      "cost =  0.6931477726957999\n",
      "cost =  0.6453290555117338\n",
      "cost =  0.64401665159208\n",
      "cost =  0.6439751350521994\n",
      "cost =  0.6439737810405846\n",
      "cost =  0.6439737363803634\n",
      "cost =  0.6439737346479629\n",
      "cost =  0.6439737343388635\n",
      "cost =  0.6439737340836315\n",
      "cost =  0.6439737338329581\n",
      "cost =  0.6439737335874941\n",
      "cost =  0.6439737333350122\n",
      "cost =  0.6439737330887974\n",
      "cost =  0.6439737328385673\n",
      "cost =  0.6439737325955438\n",
      "cost =  0.6439737323479239\n",
      "cost =  0.6439737320990867\n",
      "cost =  0.6439737318386501\n",
      "cost =  0.64397373159463\n",
      "cost =  0.6439737313358964\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwszMGnJYmJlZQ/8f024IiJluF8sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e419be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  65.5502392344\n",
      "Test Accuracy:  34.0\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    layer_dim = [12288,20,7,5,1]\n",
    "    learning_rate = 0.075\n",
    "    iteration = 2000\n",
    "    NN = NeuralNet(layer_dim)\n",
    "    NN.model(learning_rate, iteration)\n",
    "    \n",
    "    y_train_prediction = NN.predict(Type = \"train\")\n",
    "    y_test_prediction = NN.predict(\"test\")\n",
    "    \n",
    "    train_accuracy = 100 - np.mean(np.abs(y_train_prediction - NN.Y_train_org))*100\n",
    "    test_accuracy = 100 - np.mean(np.abs(y_test_prediction - NN.Y_test_org))*100\n",
    "    \n",
    "    print(\"Train Accuracy: \", train_accuracy)\n",
    "    print(\"Test Accuracy: \", test_accuracy)\n",
    "    \n",
    "main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
